`r opts_chunk$set(fig.width=7, fig.height=5, fig.path='knitr-figs/')`

# Overview of new features in dsm

## Preamble

This is a `knitr` document. The source for it contains everything you need to reproduce the analysis given here (aside from the data). The most recent version of this document can always be found at [github.com/dill/mexico-data](http://github.com/dill/mexico-data).

The data set consists of observations of dolphins off the coast of Mexico. The analysis is based on a dataset which is shipped with Distance 6.0. For convenience the data are bundled in an `R` friendly format, although all of the code necessary for creating the data from the Distance project files is available at the above URL.

The intention here is to highlight the features of the `dsm` package, rather than perform a full analysis of the data. For that reason, some important steps (such as model checking) are not fully explored.

Before we start, the necessary `R` libraries must be loaded:

``` {r}
library(ggplot2)
library(vcd)
gg.opts <- opts(panel.grid.major=theme_blank(), 
panel.grid.minor=theme_blank(), 
panel.background=theme_rect())
# maps
library(maps)
# make the results reproducable
set.seed(11123)
```

## Getting the data into `R`

### Survey area

The `R` package `maptools` can be used to load the survey region shape file.
``` {r}
library(maptools)
survey.area<-readShapeSpatial("data/Study_ar")
survey.area<-data.frame(survey.area@polygons[[1]]@Polygons[[1]]@coords)
names(survey.area)<-c("longitude","latitude")
```

Note that we ignore the file type extension in the call to `readShapeSpatial` and then we discard most of the information in the returned object, since we just require the latitude and longitude of the points which bound the survey region.

### Observation and segment data

All of the data for this analysis has been nicely pre-formatted and can be found in `data/dolphins.RData`. Loading up that data, we can see that it contains four data frames, the first few lines of each are shown:
``` {r}
load("data/dolphins.RData")
head(segdata)
head(distdata)
head(obsdata)
head(preddata)
```
`distdata` holds the distance sampling data which will be used to fit the detection function. `segdata` holds the segment data: the transects have already been "chopped" into segments. `obsdata` holds the observations which have already been aggregated to the segments and `preddata` holds the prediction grid (which includes all the covariates that we need).

Figure \ref{areawithtransects} shows the survey area with the transect lines overlaid (using data from `segdata`).

``` {r areawithtransects, fig.cap="The survey area with transect lines overlayed"}
p<-qplot(data=survey.area,x=longitude,y=latitude,geom="polygon",fill=I("lightblue"),
ylab="Latitude", xlab="Longitude", alpha=I(0.7))
p <- p + coord_equal()
p<-p+gg.opts

p <- p + geom_line(aes(longitude,latitude,group=Transect.Label),data=segdata)

print(p)
```


### Converting units

Before going any further we convert latitiude/longitude to meters. Having all of our measurements be in SI units removes the need for conversion later, making life much easier. For the survey area, we first convert to Northings and Eastings (kilometres from -88.31951 longitude, 27.01594 latitude: the centroid of the shape), then multiplying up by 1000 to get the result in metres. 
``` {r results='hide', message=FALSE}
lon0 <- -88.31951
lat0 <- 27.01594

# the dsm package has the function that we need
library(dsm)

sa.tmp <- latlong2km(survey.area$longitude, survey.area$latitude, 
lon0=lon0, lat0=lat0)

survey.area <- data.frame(x=1000*sa.tmp$km.e, y=1000*sa.tmp$km.n)

rm(sa.tmp)
```
Note that we also now name the directions `x` and `y`, to avoid confusion.

In the `segdata` frame, the `Effort` is recorded in kilometres, so just needs to be multiplied up. The latitude and longitude also need to be converted to metres:
``` {r}
segdata$Effort<-segdata$Effort*1000

seg.tmp <- latlong2km(segdata$longitude, segdata$latitude, lon0=lon0, lat0=lat0)

segdata <- cbind(segdata, x=1000*seg.tmp$km.e, y=1000*seg.tmp$km.n)
rm(seg.tmp)
```

For the `distdata` frame, only the latitude and longitude must be changed (these are only used for plotting, below):
``` {r}
ds.tmp <- latlong2km(distdata$longitude, distdata$latitude, lon0=lon0, lat0=lat0)

distdata <- cbind(distdata, x=1000*ds.tmp$km.e, y=1000*ds.tmp$km.n)
rm(ds.tmp)
```

For `obsdata`, only the effort must be converted:
``` {r}
obsdata$Effort<-obsdata$Effort*1000
```


Finally for `preddata`:
``` {r}
pred.tmp <- latlong2km(preddata$longitude, preddata$latitude, lon0=lon0, lat0=lat0)

preddata <- cbind(preddata, x=1000*pred.tmp$km.e, y=1000*pred.tmp$km.n)
rm(pred.tmp)

# find the width and height of each cell for plotting and finding the offset
lr <- c(preddata$longitude-1/6, preddata$longitude+1/6)
tb <- c(preddata$latitude-1/6, preddata$latitude+1/6)

lr.tmp <- latlong2km(lr, rep(preddata$latitude,2), lon0=lon0, lat0=lat0)
tb.tmp <- latlong2km(rep(preddata$longitude,2), tb, lon0=lon0, lat0=lat0)

preddata$width <- 1000*
(lr.tmp$km.e[(length(preddata$latitude)+1):length(lr.tmp$km.e)]-
lr.tmp$km.e[1:length(preddata$latitude)])
preddata$height <- 1000*
(tb.tmp$km.n[(length(preddata$latitude)+1):length(tb.tmp$km.n)]-
tb.tmp$km.n[1:length(preddata$latitude)])

rm(lr, tb, lr.tmp, tb.tmp)
```

Note that as well as making calculations easier, we note that using latitude and longitude when performing spatial smoothing can be problematic when certain smoother bases are used. In particular when isotropic bases are used the non-isotropic nature of latitude and longitude is inconsistent (moving one degree in one direction is not the same as moving one degree in the other).

With these quantities converted, we can now proceed to look at the data.

## Exploratory data analysis

### Distance data

The top panels of Figure \ref{EDA-plots} show histograms of observed distances and group size and the bottom panels show the relationship between observed distance and observed group size, and the relationship between observed distance and Beaufort sea state. The plots show that there is some relationship between size and observed distance to be explored since as one would expect, smaller groups seem to be seen at smaller distances and larger groups further away. We can also see the effect of sea state on observed distances.

``` {r EDA-plots, fig.height=7, fig.width=7, fig.cap="Exploratory plot of the data. Top row, left to right: histograms of distance and group size; bottom row: plot of distance against group size and plot of distances against Beaufort sea state."}
# save graphics options
o<-par("mfrow")
par(mfrow=c(2,2))

# histograms
hist(distdata$distance,main="",xlab="Distance (m)")
hist(distdata$size,main="",xlab="Group size")

# plots of distance vs. size
plot(distdata$distance,distdata$size, main="",xlab="Distance (m)",ylab="Group size",pch=19,cex=0.5,col=rgb(0.74,0.74,0.74,0.7))

# lm fit
l.dat<-data.frame(distance=seq(0,8000,len=1000))
lo<-lm(size~distance, data=distdata)
lines(l.dat$distance,as.vector(predict(lo,l.dat)))

plot(distdata$distance,distdata$beaufort, main="",xlab="Distance (m)",ylab="Beaufort sea state",pch=19,cex=0.5,col=rgb(0.74,0.74,0.74,0.7))

# restore graphics options
par(o)
```



### Spatial data

Looking separately at the at the spatial data without thinking about the distances, we can see the distribution of group size in space in Figure \ref{count-spatplot}. In the plot the size of the circles indicates the size of the group in the observation, we can see that there are rather large areas with no observations, which might cause our variance estimates to be rather large.

``` {r count-spatplot, fig.cap="The survey area. Point size is proportional to the group size for each observation."}
p<-qplot(data=survey.area,x=x,y=y,geom="polygon" , ylab="y", xlab="x", alpha=I(0.7),fill=I("lightblue"))
p<-p+gg.opts
p <- p + coord_equal()
p <- p + labs(size="Group size")
p <- p + geom_line(aes(x, y, group=Transect.Label),data=segdata)
p <- p + geom_point(aes(x, y, size=size), data=distdata, colour="red",alpha=I(0.7))
print(p)
```

We will use depth later as an explanatory covariate in our spatial model. The next plot shows the raw depth data.

``` {r fig.cap="Plot of depth values over the survey area"}
p <- ggplot(preddata)
p <- p + gg.opts
p <- p + coord_equal()
p <- p + labs(fill="Depth",x="x",y="y")
p <- p + geom_tile(aes(x=x,y=y,fill=depth, width = width, height = height))
print(p)
```

Combining the above two plots yields:
``` {r fig.cap="Plot of depth values over the survey area with transects and observations overlaid."}
p <- ggplot(preddata)
p <- p + gg.opts
p <- p + coord_equal()
p <- p + labs(fill="Depth",x="x",y="y",size="Group size")
p <- p + geom_tile(aes(x=x,y=y,fill=depth, width = width, height = height))
p <- p + geom_line(aes(x, y, group=Transect.Label),data=segdata)
p <- p + geom_point(aes(x, y, size=size), data=distdata, colour="red",alpha=I(0.7))
print(p)
```

This plot shows that we don't seem to have many observations in the very shallow aeas near the shore. This should make us skeptical of predictions in those areas.

## Estimating the detection function

Using the `ds` function in `Distance`, we can automatically select the number of adjustments that we need for the detection function.

First, loading the library:
``` {r}
suppressPackageStartupMessages(library(Distance))
```

We can then fit a detection function with half-normal key and selecting cosine adjustments using AIC (the default behaviour), we also enforce a monotonically decreasing shape for the detection function:
``` {r} 
hn.model<-ds(distdata,max(distdata$distance),monotonicity="strict")
summary(hn.model)
```
Note that the line `N in covered region` refers to groups rather than individuals (and without accounting for any spatial effects).

``` {r hn-detfct, fig.cap="Plot of the fitted detection function when a half-normal key function was used."}
plot(hn.model)
```


## Spatial analysis

Before fitting a `dsm` model, the data must be segmented; this consists of chopping up the transects and attributing counts to each of the segments. Luckily these data have already been segmented, so we can use this data as-is. In general one can segment the transects using either GIS or `R`.

The functions used to fit a spatial model can all be found in the `R` package `dsm`, which we loaded above.

### A simple model

We begin with a very simple model. We assume that the counts (of groups) in each segment are quasi-Poisson distributed and that they are a smooth function of their spatial coordinates (note that the formula is exactly as one would specify to `gam` in `mgcv`). Note that we are modelling the spatial distribution of the groups rather than individuals.

Running the model:
``` {r}
mod1<-dsm.fit(hn.model$ddf, response="group", formula=~s(x,y),
obsdata=obsdata,segdata=segdata)
summary(mod1)
```

Create the offset --  the logarithm of the area of each cell, multiplied by the probability of detection.
```{r}
off.set <- fitted(hn.model$ddf)[1]*preddata$width*preddata$height
```

We can then make the predictions over the grid we imported above.
``` {r}
mod1.pred <- dsm.predict(mod1, preddata,off=off.set)
```
Figure \ref{mod1-preds} shows a map of the predicted abundance. Before plotting, we bind on the predicitons to the data used to create them:
``` {r}
pp <- cbind(preddata,mod1.pred)
```
We can calculate abundance (of groups) over the survey area by simply summing these predictions:
``` {r}
sum(mod1.pred)
```


``` {r mod1-preds, fig.cap="Predicted density surface for `mod1`."}
p <- ggplot(pp)+gg.opts
p <- p + geom_tile(aes(x=x,y=y,fill=mod1.pred,width=width,height=height))
p <- p + coord_equal()
p <- p + scale_fill_gradientn(colours=heat_hcl(1000),limits=c(0,6))
p <- p + geom_path(aes(x=x, y=y),data=survey.area)
p <- p + labs(fill="Groups")
print(p)
```
Quartiles for the predicted values were:
``` {r}
quantile(pp$mod1.pred)
```

We can also look at diagnostic plots for the model.
``` {r mod1-check, fig.cap="Diagnostic plots for `mod1`."}
dsm.check(mod1)
```

We can calculate uncertainty in our estimates of abundance using three different methods in `dsm`, the first two use a moving block bootstrap and the third using the method proposed in Williams et al (2011). 

Using a regular moving block bootstrap takes a rather long time, but can be achieved with the following:
``` {r cache=TRUE, message=FALSE, warning=FALSE}
offset <- fitted(hn.model$ddf)[1]*pp$width*pp$height
block.size<-4
mod1.movblk <- dsm.var.movblk(mod1, preddata, 1000,
                              off.set=offset,
                              block.size=block.size,
                              ds.uncertainty=FALSE)
```
where the first argument specifies the number of bootstrap resamples, the `block.size` argument gives the size of the moving blocks and `cell.size` gives the effective area of the prediction cells. Variance estimates found using this method do not take into account uncertainty from the detection function (uncertainty in estimating the effective strip widths and hence the prediction grid areas), so to include this uncertainty the delta method must be used. Note that the delta method assumes that the two variances that it combines are independent, this assumption is clearly violated.
``` {r}
summary(mod1.movblk)
```
It is possible that there may be large outliers in the bootstrap results (as well as results which are infinite or `NA`), these are removed before calculating the variance.

To include uncertainty in the detection function we can set the `ds.uncertainty` argument to `TRUE`. This generates new distance data (using the fitted detection function) in each bootstrap re-sample and then re-fits the detection function. Re-running the bootstrap with the detection function uncertainty included can be achieved using the following code:
``` {r cache=TRUE, message=FALSE, warning=FALSE}
mod1.movblk.dsu <- dsm.var.movblk(mod1, preddata, 1000,
                                  off.set=offset,
                                  block.size=block.size,
                                  ds.uncertainty=TRUE)
```
To calculate the variances for the second method, we don't need to invoke the delta method, since the detection function uncertainty is included. Again, the summary method gives us the information we are interested in:
``` {r}
summary(mod1.movblk.dsu)
```

We can also plot a map of the coefficient of variation to help visualise the uncertainty as it varies around the area:
``` {r}
plot(mod1.movblk,limits=c(0,15))
plot(mod1.movblk.dsu,limits=c(0,15))
```
The `limits` argument allows us to put both plots on the same scale.

Alternatively, we can use the approach of Williams et al (2011), which accounts for uncertainty in the effective strip width by including its derivative in as a random effect in the model. This method is henceforth known as "variance propogation".

In order to use the method we must first re-format the data so that each row in the prediction grid and offset is an element of a list.
``` {r}
preddata.varprop <- list()
offset.varprop <- list()
for(i in 1:nrow(preddata)){
  preddata.varprop[[i]] <- preddata[i,]
  offset.varprop[[i]] <- offset[i]
}
mod1.varprop<-dsm.var.prop(mod1,pred.data=preddata.varprop,off.set=offset.varprop)
```
We can now inspect this object in the same way as with the boostrap estimates:
``` {r}
summary(mod1.varprop)
```
We can also make a plot of the CVs, as above:
``` {r}
plot(mod1.varprop,limits=c(0,15))
```

Comparing the three methods, we can see that...





### Adding another covariate to the spatial model

The data set also contains a `depth` covariate (which we plotted above). We can include in the model very simply:
``` {r}
mod2<-dsm.fit(hn.model$ddf, response="group", formula=~s(x,y)+s(depth), obsdata=obsdata,segdata=segdata)
summary(mod2)
```
Again we can plot this:
``` {r mod2-preds, fig.cap="Predicted density surface for `mod2`."}
mod2.pred<-dsm.predict(mod2, preddata,off=off.set)
pp<-cbind(preddata,mod2.pred)
p<-ggplot(pp)+gg.opts
p <- p + labs(fill="Groups")
p<-p+geom_tile(aes(x=x,y=y,fill=mod2.pred,width=width,height=height))
p <- p + coord_equal()
p <- p + scale_fill_gradientn(colours=heat_hcl(20),limits=c(0,6))
p<-p+geom_path(aes(x=x, y=y),data=survey.area)
print(p)
```

We can also look at the relationship between depth and group abundance:
``` {r mod2-depth}
plot(mod2$result,select=2)
```
Omitting the argument `select` gives plots of all the smooth terms, one at a time.

### A more complicated model

***Tweedie***

Up until this point, the response has been assumed to be quasi-Poisson. Response distributions other than the Tweedie can be used, for example the Tweedie distribution. If the Tweedie is used, then the `p` parameter must be specified. Mark Bravington (via personal communication) suggests the use of `p=1.2` for marine mammal work. The choice of `p` is only sensitive to the first decimal place, so a quick search can be performed by simply comparing the AIC of the resulting fits.
``` {r}
mod3<-dsm.fit(hn.model$ddf, response="group", formula=~s(x,y), 
		obsdata=obsdata,segdata=segdata,
		model.defn=list(fn="gam",family="Tweedie",family.pars=list(p=1.2)))
summary(mod3)
```
Note that we have to full specify the `model.defn` list here, since the default values won't be passed through otherwise.


***Soap film smoothing***

In order to use a soap film smoother for the spatial part of the model we must create a set of knots for the smoother to use. This is easily done using the `makesoapgrid()` function in `dsm`:
``` {r}
soap.knots <- make_soap_grid(survey.area,c(11,6))
```
where the second argument specifies the size of the grid that will be used to create the knots (knots in the grid outside of `survey.area` are removed).

As we saw in the exploratory analysis, some of the transect lines are outside of the survey area. These will cause the soap film smoother to fail, so we remove them:
``` {r}
onoff <- inSide(x=segdata$x, y=segdata$y, bnd=survey.area)
segdata <- segdata[onoff,]
```

We can run a model with both the `depth` covariate along with a spatial smooth utilising the soap film smoother:
``` {r cache=TRUE}
mod4<-dsm.fit(hn.model$ddf, response="group", 
formula=~s(x,y, bs="so",k=10,xt=list(bnd=list(survey.area)))+s(depth), 
obsdata=obsdata, segdata=segdata, 
model.defn=list(fn="gam",family="quasipoisson",knots=soap.knots))
summary(mod4)
```

Comparing predictions from the model that included depth, we can see that the soap film has prevented some of the extreme values (especially in the lower right corner of the survey area).

``` {r mod4-pred, fig.cap="Predicted density surface for `mod4`.", cache=TRUE}
mod4.pred<-dsm.predict(mod4,preddata, off=off.set)
pp<-cbind(preddata,mod4.pred)

p<-ggplot(pp)+gg.opts
p<-p+geom_tile(aes(x=x,y=y,fill=mod4.pred,width=width,height=height))
p <- p + coord_equal()
p <- p + scale_fill_gradientn(colours=heat_hcl(20),limit=c(0,6))
p<-p+geom_path(aes(x=x, y=y),data=survey.area)
p <- p + labs(fill="Groups")
print(p)
```

## Adding covariates to the detection function

It is common to include covariates in the detection function (so-called Multiple Covariate Distance Sampling or MCDS). In this dataset there is one covariate which is collected for each individual, that is the Beaufort sea state. We can fit hazard-rate and half-normal detection functions with the sea state included as a factor covariate as follows:
``` {r message=FALSE, cache=TRUE, warning=FALSE}
hr.beau.model<-ds(distdata,max(distdata$distance),formula=~as.factor(beaufort),
key="hr", adjustment=NULL)
summary(hr.beau.model)
hn.beau.model<-ds(distdata,max(distdata$distance),formula=~as.factor(beaufort),
key="hn", adjustment=NULL)
summary(hn.beau.model)
```
Note that we opt to not use adjustments with the covariate model since we cannot ensure that the detection function will remain monotonic if both covariates and adjustment terms are used.

The above output shows that there were some problems with fitting the half-normal (the Hessian was singular, possibly indicating that there were convergence issues). For that reason (and for variety) we use the hazard rate model.

We fit the model as above, simply changing the response to `"group.est"`:
``` {r cache=TRUE}
mod5<-dsm.fit(hr.beau.model$ddf, response="group.est", formula=~s(x,y),
obsdata=obsdata,segdata=segdata)
summary(mod5)
```
Note that in this case the response is `group.est` since we are estimating the number of groups using the Horvitz-Thompson estimator.

Note that for models where there are covariates at the individual level we cannot calculate the variance via the variance propagation method of Williams et al (2010), but rather must resort to the moving block bootstrap. Other than this, all of the above models can be fitted.

A plot of predicitons from the covariate model:
``` {r mod5-pred, fig.cap="Predicted density surface for `mod5`."}
mod5.pred<-dsm.predict(mod5,preddata, off=off.set)
pp<-cbind(preddata,mod5.pred)

p <- ggplot(pp)+gg.opts
p <- p + geom_tile(aes(x=x,y=y,fill=mod5.pred,width=width,height=height))
p <- p + coord_equal()
p <- p + scale_fill_gradientn(colours=heat_hcl(20),limit=c(0,6))
p <- p + geom_path(aes(x=x, y=y),data=survey.area)
p <- p + labs(fill="Groups")
print(p)
```

## Conclusions

This document has hopefully shown that the `dsm` package is a versatile and relatively easy-to-use package for the analysis of spatial distance sampling data.


## References and notes
 * `maptools` relies on GDAL, instructions for how to get this to work on Mac OS can be found at [here](http://lostingeospace.blogspot.com/2011/08/rgeos-on-macos-x.html).
 * `Distance` is available at [http://github.com/dill/Distance](http://github.com/dill/Distance) as well as on CRAN.
 * `dsm` is available (along with some documentation and hints) at [http://github.com/dill/dsm](http://github.com/dill/dsm).
